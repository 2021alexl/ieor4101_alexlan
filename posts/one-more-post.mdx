---
title: Applied Statistics
description: "This module includes point & interval estimation, hypothesis testing, and regressions."

---
# Overview

Please see this table about informations covered in this module

This module will not cover elementary knowledge of statistics, like frequency distribution, sampling methods, etc.


|        | Description                                                               |
| ------ | ------------------------------------------------------------------------- |
|  1     | Estimation                                                                |
|  2     | Testing of Statistical Hypothesis                                         |
|  3     | Simple Linear Regrssion and Correlation                                   |
|  4     | Other Tests and Analysis of Variance                                      |

# 1 Estimation

## 1.1 Sampling

Usually, there are some numerical characteristics about the population which the investigator wants to know. Such numerical facts are called parameters; e.g., the population size, population mean, a proportion of some attribute, and variability in the population, etc.

> A Parameter is a numrical value, or a characteristc, of a population.

The parameters cannot be determined exactly, but can only be estimated from a sample by quantities
called statistics.

> A Statistic is a numerical summary, or a characteristc, of a sample.

## 1.2 Point Estimation

A point Estimate is that value of a statistic, which has been calculated from a sample, that estimates a parameter of the population.

Two methods of Estimation we often adopt: 

1. The method of moments 

2. Maximum Likelihood method

### 1.2.1 The Method of Moments

Let X1, X2, ..., Xn be a random sample of a random variable X. The average value of the kth power of X1, X2, ..., Xn:

![](https://latex.codecogs.com/svg.image?M_{k}&space;=&space;1/n&space;\sum_{i=1}^{n}&space;X_{i}^{k})


is called the kth sample moment, for k = 1, 2, 3, ... . while E [X^k] is called kth population moment, for k = 1, 2, 3, ... . Thus the method of moments estimators of the parameters is given by setting the sample moments equal to population moments and solving the resulting equations simlutaneously, for the parameters of the population.

### 1.2.2 The Maximum Likelihood estimators

The essential feature of the principle of maximum likelihood estimation is that it requires the investigator to choose as an estimate of the parameter that value of the parameter for which there is the apriori probability of obtaining the sample point actually observed, is as large as possible. This probability will in general depend on the parameter, which is then given that value for which this probability is as large as possible.

Suppose that the population random variable X has a probability function which depends on some parameter θ =P(X=x) = f (x; θ). We suppose that the form of the function f is known, but not the value of θ. The joint probability function of the sample random variables, evaluated at the sample point ( x1, x2, ..., xn), is

![](https://latex.codecogs.com/svg.image?L(\theta&space;)=&space;f&space;(X1,&space;X2,&space;...,&space;Xn;&space;\Theta&space;)&space;=&space;\prod_{1}^{n}&space;f(x_{i};\Theta&space;))

This function is also known as the likelihood function of the sample. We are considering it as a function of θ when the sample values x1, x2, ..., xn are fixed.

The principle of maximum likelihood requires us to choose as an estimate of the unknown parameter that value of θ for which the likelihood function assumes its maximum value.

### 1.2.3 Statistics as Estimators for Parameters

### 1.2.3.1 The sample Proportion

> On the first day of the semester, in Stat 1350, asking a class of 30 students the following question: Who has a calculator? 12 students raised their hands by showing that they have a calculator. Estimate the percentage of the students, in this class, that have a calculator.

> By the definition of sample porportion, n = 30 and x = 12, p = 12/30 = 40%

### 1.2.3.2 The sample mean and sample wariance

Sample mean:

![](https://latex.codecogs.com/svg.image?\bar{X}&space;=&space;1/n&space;\sum_{i=1}^{n}&space;X_{i})

Sample variance:

![](https://latex.codecogs.com/svg.image?S^2&space;=&space;\sum_{i=1}^{n}(x_{i}-\bar{X})^2/(n-1))

### 1.2.4 Properties of the Estimators and their Sampling Distributions

#### 1.2.4.1 sampling distribution of pˆ (p hat)

For a simple random sample of size n such that n is at most 0.05N:

> The shape of the sampling distribution of p^ is approximately normal provided np(1-p) is at least 10

> The mean of the sampling distribution is p

> the standard deviation of sampling distribution of p^ is 

> ![](https://latex.codecogs.com/svg.image?s_{p}&space;=&space;\sqrt{p(1-p)/n})

#### 1.2.4.2 

Weak Law of Large Numbers (WLLN): 

> The Law of Large Numbers: As additional observations are included in the sample, the difference between the statistic X bar (the sample mean) and the parameter μ (the population mean) approaches 0

![](https://latex.codecogs.com/svg.image?\displaystyle&space;\lim_{n&space;\to&space;\infty&space;}&space;[P(|X_{n}-\mu&space;|)\geq&space;\varepsilon&space;]&space;=&space;0)

In addition, when n increases, the sample mean becomes more close to the population mean, and the standard deviation of the sampling distribution decreases (as n increases). Hence, we have **Central Limit Theorem**:

> Regardless of the shape of the population, whether it was normal or not, the sampling distribution of X bar becomes approximately normal as n increases with mean and variance as specified above


## 1.3 Interval Estimation (Confidence Interval)

Instead of reporting our estimation about some characteristics of the population by using X bar, we can report the estimation using an interval, like X bar +/- E, where E stands for the error. Technically, E is called margin of error.

### 1.3.1 Confidence Interval about One Proportion

Based on our knowledge about binomial distribution, we know that the p^ = X/n, E(x) = np, and Var(x) = np(1-p). By the CLT, the random variable given

![](https://latex.codecogs.com/svg.image?Z&space;=&space;(\hat{p}-p)&space;/&space;\sqrt{p(1-p)n})

has a standard normal distribution as n increases, where Z is the value of the standard normal variable comprising a probability of alpha on its right:

![](https://latex.codecogs.com/svg.image?P(-Z_{\alpha&space;/2}<&space;Z&space;<&space;Z_{\alpha&space;/2}&space;)&space;=&space;1-\alpha&space;)

And we get the confidence interval on p given by:

![](https://latex.codecogs.com/svg.image?\hat{p}&space;-Z_{\alpha&space;/2}\sqrt{\hat{p}(1-\hat{p}/n)}&space;<&space;p&space;<\hat{p}&space;&plus;&space;Z_{\alpha&space;/2}\sqrt{\hat{p}(1-\hat{p}/n)})

### 1.3.1 Confidence Interval about One Mean

#### 1.3.1.1 When the variance of population is known (case 1)

In this case the random variable 

![](https://latex.codecogs.com/svg.image?Z&space;=&space;\bar{X}-u/s/\sqrt{n})

has a N(0,1) distribution and 

![](https://latex.codecogs.com/svg.image?P(-Z_{\alpha&space;/2}<&space;Z&space;<&space;Z_{\alpha&space;/2}&space;)&space;=&space;1-\alpha&space;)

and the confidence interval on population mean u is given by:

![](https://latex.codecogs.com/svg.image?\bar{x}-Z_{\alpha&space;/2}&space;\cdot&space;s/\sqrt{n}&space;<&space;\mu&space;<\bar{x}&space;&plus;&space;Z_{\alpha&space;/2}&space;\cdot&space;s/\sqrt{n})

The above interval is called z-interval about the mean. 

#### 1.3.1.2 When the variance of population is unknown, but sample size is bigger than 30 (or 50) (case 2)

In this case, We estimate the population standard deviation by the sample standard deviation. By replacing σ by s, we get 

![](https://latex.codecogs.com/svg.image?\bar{x}-Z_{\alpha&space;/2}&space;\cdot&space;s/\sqrt{n}&space;<&space;\mu&space;<\bar{x}&space;&plus;&space;Z_{\alpha&space;/2}&space;\cdot&space;s/\sqrt{n})


#### 1.3.1.3 When the variance of population is unknown, and sample size is small, maybe less than 30 (or 50) (case 3)

In this case, we do not have condition to do point estimation about population variance, therefore we apply a T interval to construct CI about population mean:

![](https://latex.codecogs.com/svg.image?T&space;=&space;\bar{X}-u/s/\sqrt{n})

This random variable will have a t distribution with n-1 degrees of freedom. T distribution behaves like standard normal distribution and as n increases, T becomes very close to the standard normal distribution:

![](https://latex.codecogs.com/svg.image?P(-t_{\alpha&space;/2}<t<t_{\alpha&space;/2}&space;)&space;=&space;1-\alpha&space;)

and CI on u:

![](https://latex.codecogs.com/svg.image?\bar{x}-t_{\alpha&space;/2}&space;\cdot&space;s/\sqrt{n}&space;<&space;\mu&space;<\bar{x}&space;&plus;&space;t_{\alpha&space;/2}&space;\cdot&space;s/\sqrt{n})

### 1.3.3 Confidence Interval about One Variance


From a random sample of size n; X1, X2... Xn, taken from that normal population, we can see that:

![](https://latex.codecogs.com/svg.image?S^2&space;=&space;1/(n-1)\sum_{i=1}^{n}(X_{i}-&space;\bar{X})^2)

is the sample variance. Thus the random variable given by

![](https://latex.codecogs.com/svg.image?&space;\chi&space;^2&space;=&space;(n-1)S^2/s^2)

and it will have a chi-square distribution with n-1 dof:

![](https://latex.codecogs.com/svg.image?P(\chi&space;_{1-\alpha&space;/2}^2&space;<&space;\chi&space;<&space;\chi&space;_{\alpha&space;/2}^2)&space;=&space;1-\alpha&space;)

and therefore we have the CI for the pop variance given by

![](https://latex.codecogs.com/svg.image?(n-1)s^2/\chi&space;_{\alpha&space;/2}^{2}&space;\leq&space;\sigma&space;^2&space;\leq&space;(n-1)s^2/\chi&space;_{1-\alpha&space;/2}^{2})

> Example: Human beings vary in the time it takes them to respond to driving hazards. In one experiment in which 100 healthy adults between age 21 and 30 years were subjected to a certain driving hazard, and the sample variance of the observed times it took them to respond was 0.0196 second squared. Assuming that the times to respond are normally distributed, estimate the variability in the time response of the given age group using a 95% C.I.

> ANS: The confidence level is 0.95, so the alpha/2 = 0.025. chi-sq table with 100-1 = 99 dof is chi-sq(0.025) = 128.45, chi-sq(0.975) = 128.45, so we have the following interval: 0.0151 < σ2 < 0.0265.

# 2 Testing of Statistical Hypothesis

### The idea of innocent until proven guilty

When we conjecture a statement about a parameter or population, or a broader statement, and we want to prove that it is correct, the only way to prove our conjecture is true is to prove that the alternative conjecture, which is the total opposite & mutually exclusive case of the statement, is statistically unlikely to be true. 

For example: if I want to convince other people my statement that the 550 ml bottle water produced by Poland Spring is actually less than 550 ml, I need to prove that the fact "Poland Spring's 550 ml bottle water on average is equal to or above 550 ml" is not statistically likely to happen. 

The procedure is called hypothesis test, in this case, I made a null hypothesis (called H0) and an alternative hypothesis (called Ha). For most of the cases, the one that I want to purpose (I wish to become correct from the bottom of my heart) is the alternative hypothesis, and the Null Hypothesis is the "mutually exclusive and opposite" thing.

> H0: the Poland Water bottle water on average is has no less than 550ml water

> Ha: the Poland Water bottle water on average is has less than 550ml water

### 2.1 Types of errors

There are two types of errors:

**Type I Error**: A Type I error has been committed if the test rejects the null hypothesis when in fact it is true. The probability of making such an error will be denoted by α, (The Greek letter Alpha). For sure, it is clear that 0 ≤α≤ 1.

**Type II Error**: A Type II error has been committed if the test does not reject H0 when H0 is false. The probability of making such an error will be denoted by β, (the Greek letter Beta) with 0 ≤β≤ 1. What is more important is that we do not like to make such errors with high probabilities.

**Level of Significance**: The probability of committing a type I error is denoted by (Alpha) α. It is called the theoretical level of significance for the test. The most common used values for α are: 0.01, 0.05, or 0.10. Other values for α are at the discretion of the researcher. 

More expressions for α are:

α = P (committing a type I error) = P (rejecting H0 when H0 is true) = P (rejecting H0 when H1 is false).

**The probability of committing a type II error** is denoted by (beta) β. 

Other labels for β are:

β = P (committing a type II error) = P (not rejecting H0 when H0 is false) = P (not rejecting H0 when H1 is true)

**Power of the Test**: The value of 1– β, which stands for P (rejecting H0 when H0 is false) is the power of the test

> Commonly asked Question: which error is more dangerous?

> Type 1, suppose H0 is this man is not guilty and Ha is that this man is guilty, the type 2 error is decide a guilty man to be innocent, which is relatively more tolerable than type 1 error which is to decide a innocent man guilty

### 2.2 Rejection Region

the rejection region is an interval which determined by the distribution and the level of significance you choose. If it falls in the reject region, the H0 hypothesis is deemed to be **too unlikely to happen**, and therefore we reject H0 (not actually meaning we accept Ha, in fact, I have been doing stats for 5 years and I never saw a statement saying Ha is accepted)

**General Conclusion**

> Reject H0 if the computed value of the test statistic falls in the reject region.

> Do not reject H0 if the computed value of the test statistic does not fall inside the reject region.

### 2.3 Steps

There are generally two methods 1) Classical Method 2) P-Value Method

#### 2.3.1 Classical Method Steps

After determining H0 and Ha and significance value alpha, you want to:

1. Deccide the critical value and locate the rejection region -- the detailed procedure is depending on the tails: 

- if your alternative hypothesis is the sign *>*, you want to locate the right hand side (your critical value should be a positive number)

- if your alternative hypothesis is the sign *<*, you want to locate the left hand side (your critical value should be a negative number)

- if your alternative hypothesis is the sign *!=*, you want to devide the alpha by two and locate both sides (your critical value should be a positive number)

2. Compute statistics

3. Make Statistical decision and give conclusion

#### 2.3.2 P-Value Method Steps

After determining H0 and Ha and significance value alpha, you want to:

1. Compute statistics and determine the p-value, which is an attained significance level based on your test's distribution

2. compare p-value and alpha, a p-value less than alpha will lead rejection in H0, no matter which tail it is

3. Make Statistical decision and give conclusion

### 2.4 Hypothesis testing with one parameter

#### 2.4.1 Hypothesis testing with one proportion (p)

the estimated proportion (result from MLE) is the sample proportion: 

> p = x/n

where x is the number of individuals in the sample w/ specified characteristics and n is the sample size.

to calculate standard deviation, we use the formula:

![](https://latex.codecogs.com/svg.image?s_{\hat{P}}&space;=&space;\sqrt{p(1-p)/n})

and it has to follow these three requirements:

- random sample
-np(1-p) > 10
-idependent sample values

There are three ways to set up the null and alternative hypothesis:

a) Equal hypothesis versus not equal hypothesis: H0: P = P0 versus H1: P != P0, two-tailed test.

b) At least versus less than: H0: P ≥ P0 versus H1: P < P0, left-tailed test.

c) At most versus greater than: H0: P ≤ P0 versus H1: P > P0, right-tailed test.

For the case (a), we want to use 

![](https://latex.codecogs.com/svg.image?|Z|&space;>&space;Z_{\alpha&space;/2})

as the reject region

For the case (b), we want to use 

![](https://latex.codecogs.com/svg.image?Z&space;<&space;-Z_{\alpha&space;})

as the reject region

For the case (c), we want to use 

![](https://latex.codecogs.com/svg.image?Z&space;>&space;Z_{\alpha&space;})

as the reject region

To calculate the test statistic, we use

![](https://latex.codecogs.com/svg.image?Z=(p-p_{0})/[\sqrt{p_{0}(1-p_{0}/n)}]&space;=&space;(x-np_{0})/\sqrt{np_{0}(1-p_{0})}])

Or if you want to use P method: 

For the Left tail test:

![](https://latex.codecogs.com/svg.image?p_{value}&space;=&space;P(Z<Z_{calculated}))


For the Right tail test:

![](https://latex.codecogs.com/svg.image?p_{value}&space;=&space;P(Z>Z_{calculated}))


For the two tail test:

![](https://latex.codecogs.com/svg.image?p_{value}&space;=&space;P(Z>Z_{calculated})&space;*&space;2)

If your calculated Z is 2.965 and you are doing a right tail test, use CDF to calculate or use table to get P(Z > 2.65)

#### 2.4.2 Hypothesis testing with one mean (u)

##### 2.4.2.1 When the population variance is known or is unknown but large enough sample size

![](https://latex.codecogs.com/svg.image?Z&space;=&space;(\bar{x}-\mu&space;)/s/\sqrt{n})

use the equation above to get the test statistic and use procedure stated in 2.4.1 to reach your conclusion

##### 2.4.2.2 When the population variance is unknown & small sample size

![](https://latex.codecogs.com/svg.image?T&space;=&space;(\bar{x}-\mu&space;)/s/\sqrt{n})

We want to use this equation to calculate test statistics and we wanna search the T table 


### 2.5 Hypothesis Testing Concerning Two Parameters

#### 2.5.1 Two proportions


